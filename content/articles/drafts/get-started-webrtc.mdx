---
title: Туториал по WebRTC
date: 2025-06-28
tags:
  - webrtc
description: >
  WebRTC это, пожалуй, одна из самых интересных технологий, с которой мне довелось работать.

  В данном туториале мы рассмотрим основы работы с WebRTC, а также создадим простое приложение для обмена видео и аудио между двумя браузерами.
visualized: true
---

<script>
	import WRTCVideo from '$mod/webrtc/WRTCVideoCapture.svelte'; import Information from
	'$lib/components/Content/ContentInformation.svelte'; import CodeOutput from
	'$components/Content/ContentCodeOutput.svelte'; import WRTCScreen from
	'$mod/webrtc/WRTCScreenCapture.svelte'; import Dial from '$mod/animation-entity/Dial.svelte';
	import Excalidraw from '$components/Content/ContentExcalidraw.svelte';
</script>

> **Дисклеймер**:
>
> Данный туториал будет запрашивать доступ к вашим медиа-устройствам (камере и микрофону). Пожалуйста, убедитесь, что вы понимаете риски, связанные с предоставлением такого доступа, и используйте его только в безопасной среде.
> Данные, которые вы предоставите, не будут сохраняться и/или передаваться по сети Интернет и будут использоваться только в рамках данного туториала.

**WebRTC** - это технология, которая позволяет браузерам и мобильным приложениям обмениваться аудио и видео в реальном времени без необходимости использования промежуточных серверов. Это делает WebRTC идеальным выбором для приложений, которым необходим функционал видеозвонков, стриминга, и аудиовызовов.

По сути, WebRTC состоит из двух основных частей:

1. Методы захвата видео и аудио потоков;
2. Методы передачи этих потоков между клиентами;

Разберем эти части подробнее.

<Dial />

# Захват потоков

В целом про аудио- и видео-захват можно говорить в контексте потоков, так как и те, и другие данные приходят нам в режиме реального времени.

Для начала, давайте дадим определение термину "поток":

<Information title="Что такое поток?">

**Поток** - это последовательность данных, которая может быть обработана по частям, а не целиком. В контексте WebRTC, поток может представлять собой аудио или видео данные, которые передаются в реальном времени и обрабатываются частями.

Современным примером потока являются ответы ИИ. Практически все современные ИИ не генерируют ответ текста целиком и сразу, они производят генерацию по токенам, передавая получившийся результат по частям. Это позволяет пользователю видеть ответ быстрее, а также дает возможность прервать генерацию, если ответ не удовлетворяет ожиданиям.

</Information>

В контексте WebRTC у нас есть два основных типа потоков:

1. **Аудио поток** - это последовательность аудио данных, которые могут быть захвачены с микрофона пользователя или получены из другого источника.
2. **Видео поток** - это последовательность видео данных, которые могут быть захвачены с камеры пользователя или получены из другого источника (например, демонстрация экрана).

Все устройства, которые могут захватывать такие данные называются **медиа-устройствами**.

В браузере мы можем получить доступ к этим устройствам с помощью API `MediaDevices`, которое позволяет нам следить за изменением списка таких устройств, запрашивать доступ к данным подключенных устройств и многое другое.

Для того чтобы запросить доступ к медиа-устройствам, мы можем использовать метод `getUserMedia`, который возвращает промис с объектом `MediaStream`. Этот объект представляет собой поток аудио и/или видео данных:

```javascript
navigator.mediaDevices
	.getUserMedia({ audio: true, video: true })
	.then((stream) => {
		// Здесь мы можем использовать полученный поток
		console.log('Получен поток:', stream);
	})
	.catch((error) => {
		console.error('Ошибка при получении медиа-потока:', error);
	});
```

После выполнения данного кода, браузер запросит у пользователя разрешение на доступ к камере и микрофону. Если пользователь согласится, мы получим объект `MediaStream`, который содержит аудио и видео данные.

В сниппете вверху также стоит обратить внимание на объект, который мы передаем в `getUserMedia`. Данный объект позволяет нам указать, какие именно данные мы хотим получить: аудио, видео или оба потока сразу. В данном случае мы запрашиваем и аудио, и видео.

Пока что, это все что мы должны знать про захват потоков. После того, как мы получили потоки, мы можем делать с ними все, что угодно: воспроизводить их в `<video>` или `<audio>` элементах, обрабатывать с помощью Web Audio API, и т.д.

Для примера, давайте выведем картинку с камеры через видео-поток в элемент `<video>`:

```html filename="index.html"
<video autoplay playsinline controls="false" id="cam-video-stream"></video>
```

```javascript filename="index.js"
const videoEl = document.getElementById('cam-video-stream');

try {
	const constraints = { video: true };
	const stream = navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
		videoEl.srcObject = stream;
	});
} catch (error) {
	console.error('Error opening video stream from camera.', error);
}
```

В результате мы должны получить поток видео прямо с камеры. Вот реализованый пример, который можно потыкать:

<WRTCVideo class="mt-4" />

<Information title="Атрибуты у элемента video">

Обратите внимание на то, какие атрибуты мы указали у элемента `<video>`:

- `autoplay` - позволяет видео начинать воспроизводиться автоматически, как только поток будет готов;
- `playsinline` - позволяет видео воспроизводиться встраиваемым образом, без перехода в полноэкранный режим на мобильных устройствах;
- `controls="false"` - отключает стандартные элементы управления видео, чтобы мы могли управлять воспроизведением самостоятельно, если это потребуется.

</Information>

## Устройства

До текущего момента мы рассматривали только захват потоков со стандартных медиа-устройств. В большинстве приложений есть селект для выбора устройства, с которого будет производиться захват. Например, если у пользователя есть несколько камер или микрофонов, он может выбрать нужное устройство.

Для того чтобы получить все медиа-устройства, к которым браузер имеет доступ, мы можем использовать метод `enumerateDevices` из API `MediaDevices`. Этот метод возвращает промис с массивом объектов `MediaDeviceInfo`, которые содержат информацию о каждом устройстве:

```javascript
navigator.mediaDevices.enumerateDevices().then((devices) => {
	console.log('Список медиа-устройств:', devices);
});
```

```json
// Вывод
Список медиа-устройств:
[
  {
    "deviceId": "default",
    "kind": "audioinput",
    "label": "Default - Микрофон MacBook Pro (Built-in)",
    "groupId": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  {
    "deviceId": "efd16f99b50d66de56fd4a2b247a5bb4cc64feee924a99879e08486834355dbb",
    "kind": "audioinput",
    "label": "Микрофон MacBook Pro (Built-in)",
    "groupId": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  {
    "deviceId": "871a76159da73ea7eb2bfa3cd39ad555692476a29d5503b3b939c7ab2700ff1b",
    "kind": "videoinput",
    "label": "HD-камера FaceTime",
    "groupId": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  // ...
]
```

Также, стоит учитывать, что пользователь может подключить камеру/микрофон или любое другое медиа-устройство в процессе работы приложения. Поэтому, для того чтобы отслеживать изменения в списке устройств, мы можем подписаться на событие `devicechange`:

```javascript
let devices = [];

// Получаем список медиа-устройств при загрузке страницы
navigator.mediaDevices.enumerateDevices().then((newDevices) => {
	devices = newDevices;
	console.log('Список медиа-устройств:', devices);
});

// Устанавливаем обработчик события, который будет вызываться при изменении доступного списка устройств
navigator.mediaDevices.addEventListener('devicechange', () => {
	navigator.mediaDevices.enumerateDevices().then((newDevices) => {
		devices = newDevices;
		console.log('Обновленный список медиа-устройств:', devices);
	});
});
```

## Выбор устройства и объект ограничений

Ранее, при вызове `getUserMedia`, мы указывали объект который содержал желаемые потоки:

```javascript
// Указываем какие именно потоки мы хотим получить
const constraints = { audio: true, video: true };
navigator.mediaDevices.getUserMedia(constraints);
```

Данный объект часто называют **ограничением потоков** (constraints), ибо он (в простой версии) позволяет ограничить используемые потоки. По умолчанию `getUserMedia` вернет нам потоки с устройств, которые выбраны по умолчанию в настройках браузера, однако, объект ограничений позволяет нам указать не только конкретные потоки, но и ограничить устройства, с которых будут писаться потоковые данные:

```javascript
const devices = await navigator.mediaDevices.enumerateDevices();
const streams = await navigator.mediaDevices
	.getUserMedia({
		audio: {
			deviceId: devices[0].deviceId // Указываем конкретное устройство для аудио
		},
		video: {
			deviceId: devices[1].deviceId // Указываем конкретное устройство для видео
		}
	})
	.catch((error) => {
		console.error('Ошибка при получении медиа-потока:', error);
	});
```

<Information title="Спецификация">

В процессе написания данной статьи я часто заглядывал в спецификацию W3C, которая описывает работу с медиа-потоками. Если вам интересно, то вы можете ознакомиться с ней [здесь](https://www.w3.org/TR/mediacapture-streams/).

Что касательно объекта ограничений, то он описан в [спецификации Media Capture and Streams](https://www.w3.org/TR/mediacapture-streams/#dom-mediastreamconstraints), где он называется `MediaStreamConstraints`. В спецификации также описаны все возможные свойства, которые можно использовать в этом объекте.

</Information>

Помимо того, что мы можем указать конкретные устройства, мы также можем указать другие параметры, такие как разрешение видео, частота кадров и т.п.
По возможности, всегда стоит уточнять лучшие параметры для захвата видео и аудио, так как это может значительно улучшить качество получаемых данных.

```javascript
const constraints = {
	audio: {
		deviceId: 'default', // Используем устройство по умолчанию для аудио
		echoCancellation: true // Включаем подавление эха
	},
	video: {
		width: { min: 600, ideal: 1280 }, // Минимальная и идеальная ширина видео
		height: { min: 400, ideal: 720 }, // Минимальная и идеальная высота видео
		frameRate: { ideal: 30 } // Идеальная частота кадров
	}
};
```

## Захват экрана

Для захвата экрана мы можем использовать метод `getDisplayMedia`, который работает аналогично `getUserMedia`, но позволяет захватывать содержимое экрана или отдельных окон/вкладок:

```html filename="index.html"
<video autoplay playsinline controls="false" id="cam-video-stream"></video>
```

```javascript filename="index.js"
const videoEl = document.getElementById('cam-video-stream');

try {
	const constraints = { video: true };
	// [!code highlight]
	const stream = navigator.mediaDevices.getDisplayMedia(constraints).then((stream) => {
		videoEl.srcObject = stream;
	});
} catch (error) {
	console.error('Error opening video stream from camera.', error);
}
```

<WRTCScreen class="mt-4" />

## Треки потока

**Трек** (_они же дорожки_) - это отдельный поток данных, который может быть аудио или видео.

Когда мы получаем поток с помощью `getUserMedia`, который содержит как аудио, так и видео данные, мы можем получить доступ к отдельным трекам этого потока. В случае если мы запросили только аудио или только видео, то у нас будет только один трек.

Треки мы можем получить из объекта `MediaStream` с помощью свойства `getTracks()`, которое возвращает массив всех треков в потоке:

```javascript
const constraints = { video: true };
const stream = await navigator.mediaDevices.getUserMedia(constraints);
console.log(stream.getTracks());
```

```json
[
	{
		"contentHint": "",
		"enabled": true,
		"id": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
		"kind": "video",
		"label": "HD-камера FaceTime",
		"muted": false,
		"oncapturehandlechange": null,
		"onended": null,
		"onmute": null,
		"onunmute": null,
		"readyState": "live"
	}
	// <При условии, если мы запросили бы аудио, то тут был бы второй трек>
	// ...
]
```

Мы можем управлять каждым треком отдельно, например, отключать или включать его:

```javascript
const tracks = stream.getTracks();
// Отключаем трек
tracks[0].enabled = false;
// Включаем трек
tracks[0].enabled = true;
```

Или же мы и вовсе можем остановить все треки. К слову кнопка "Прекратить трансляцию" в примерах выше, сделана следующим образом, данный сниппет останавливает все треки в потоке:

```javascript
stream.getTracks().forEach((track) => track.stop());
```

<Information title="Спецификация">

Сами треки в спецификации называются `MediaStreamTrack`, их можно найти [вот тут](https://www.w3.org/TR/mediacapture-streams/#tracks-and-constraints).

</Information>

# Соединение

Каждое соединение в WebRTC состоит из трех основных частей:
1. **Сигналинг** нужен для того, чтобы установить первоначальное соединение между клиентами;
2. Создание **Peer Connection** нужно для создания объекта соединения, который будет использоваться для обмена данными;
3. **Обмен офферами** нужен для обмена информацией о медиа-потоках и установления соединения;

Внизу мы рассмотрим каждый из этих этапов более подробно.

## Сигналинг
**Сигналинг** - это процесс обмена сообщениями между клиентами для установления соединения, обмена информацией о медиа-потоках, а также для передачи других данных, необходимых для работы WebRTC.

Для начала стоит сказать, что до того как два клиента смогут обмениваться аудио и видео данными, им нужно установить соединение друг с другом, для того чтобы они могли обмениваться сообщениями о предстовящем соединении (качество видео, адреса клиентов, и прочее).

WebRTC не диктует нам, какую именно технологию мы должны использовать для обмена этими сообщениями, но чаще всего используется соединение через WebSocket.
Сам процесс установления первичного соединения между двумя клиентами называется **сигналинг**.

<Excalidraw src="/articles/webrtc-get-started/webrtc-signaling.svg" />

## Что такое Peer Connection?

**Peer Сonnections** — это часть технологии WebRTC, которая позволяет двум приложениям (например, браузерам на разных компьютерах) напрямую обмениваться видео, аудио или бинарными данными.

Такое соедиенение работает по принципу “peer-to-peer” (от клиента к клиенту), без центрального сервера для передачи данных.

Для того чтобы создать объект соединения, с помощью которого (в будущем) мы сможем передавать сообщения между клиентами, нам нужно создать объект, который будет удовлетворять интерфейсу `RTCPeerConnection`. Этот объект будет содержать необходимые ICE-сервера, которые будут использоваться для соединения.

> Не бойтесь аббревиатуры `ICE`, в следующем разделе мы подробно рассмотрим, что это такое и зачем оно нужно.
>
> Пока что стоит знать, что для установления соединения, нам нужно указать хотя бы один ICE-сервер

```javascript
// Конфигурация ICE-серверов
const configuration = { iceServers: [{ urls: 'stun:stun.l.google.com:19302' }] };

// Инициализация объекта RTCPeerConnection
const peerConnection = new RTCPeerConnection(configuration);
```

<Information title="Спецификация">

Объект `RTCPeerConnection` описан в [спецификации WebRTC](https://www.w3.org/TR/webrtc/#rtcpeerconnection-interface).

</Information>

## ICE

**ICE (Internet Connection Establishment)** - это процесс установления пути, по которому два клиента смогут связаться с друг-другом, даже если они находятся за NAT или файрволом.

Сам процесс ICE обычно подразумевает под собой получения или передачу данных на один из двух серверов ниже:

1. **STUN (Session Traversal Utilities for NAT)** - серверы, которые помогают клиентам определить свой публичный IP-адрес и порт, а также позволяют клиентам обмениваться этой информацией друг с другом.
2. **TURN (Traversal Using Relays around NAT)** - серверы, которые могут использоваться для передачи данных между клиентами, если прямое соединение не удалось установить. TURN серверы выступают в роли промежуточного узла, через который проходят данные.

<Excalidraw src="/articles/webrtc-get-started/wrtc-ice-servers.svg" />

По сути данные сервера просто отдают информацию о том, какие IP-адреса и порты доступны для соединения у клиента, а также помогают клиентам обмениваться этой информацией друг с другом.

<Excalidraw src="/articles/webrtc-get-started/ice-gathering.svg" />

В сниппете кода с созданием `RTCPeerConnection` из раздела выше мы указали только один STUN-сервер, который будет использоваться для установления соединения.

В реальных приложениях обычно используется несколько STUN и TURN серверов, чтобы обеспечить надежное соединение в различных сетевых условиях.

Популярными (и бесплатными) вариантами для получения возможных путей для соединения являются следующие STUN-серверы:

- `stun.l.google.com:19302`
- `stun.speedy.com.ar:3478`
- `stun.nextcloud.com:443`
- `stun.ideasip.com:3478`
- `stun.imesh.com:3478`
- `stun.infra.net:3478`

## Офферы
